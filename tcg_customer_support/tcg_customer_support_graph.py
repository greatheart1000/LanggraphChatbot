"""
TCG å®¢æœåœºæ™¯å¤„ç† LangGraph
ä¸º 14 å¤§ç±»åœºæ™¯çš„æ¯ä¸ªå­ç±»åˆ†åˆ«åˆ›å»ºå¤„ç†æµç¨‹
"""
from typing import Annotated, TypedDict, Literal, List, Dict, Any
from pathlib import Path
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.checkpoint.memory import MemorySaver
import json
import os

# å¯¼å…¥é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
try:
    from config import OPENAI_API_KEY, OPENAI_BASE_URL, OPENAI_MODEL, SCENARIOS_FILE, SOP_BASE_DIR
except ImportError:
    # é»˜è®¤é…ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "sk-lcfvUUrmDih6qQWW5eC89504A869464d91E2AbFaBe087d43")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "http://one-api.internal-tools.com/v1")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
    # ä½¿ç”¨ç›¸å¯¹äºå½“å‰æ–‡ä»¶çš„è·¯å¾„ï¼Œè‡ªåŠ¨æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    _current_dir = Path(__file__).parent
    _root_dir = _current_dir.parent.parent  # ä» æ™ºèƒ½å®¢æœç³»ç»Ÿ/tcg_customer_support åˆ°æ ¹ç›®å½•
    _default_scenarios_file = _root_dir / "TCG å®¢æœåœºæ™¯flow_parsed.json"
    SCENARIOS_FILE = os.getenv("SCENARIOS_FILE", str(_default_scenarios_file))
    SOP_BASE_DIR = os.getenv("SOP_BASE_DIR", str(_current_dir.parent / "sop_data_global_en"))

# å®šä¹‰çŠ¶æ€
class CustomerSupportState(TypedDict):
    """å®¢æœåœºæ™¯å¤„ç†çŠ¶æ€"""
    messages: Annotated[List[AnyMessage], add_messages]
    user_query: str
    category: str  # å¤§ç±»åœºæ™¯
    subcategory: str  # å­ç±»åœºæ™¯
    context: Dict[str, Any]  # ä¸Šä¸‹æ–‡ä¿¡æ¯
    response: str  # å“åº”å†…å®¹
    next_action: str  # ä¸‹ä¸€æ­¥åŠ¨ä½œ
    history: List[Dict]  # å¤„ç†å†å²

# åˆå§‹åŒ– LLM - ä½¿ç”¨è‡ªå®šä¹‰ base_url å’Œ api_key
llm = ChatOpenAI(
    model=OPENAI_MODEL,
    temperature=0,
    api_key=OPENAI_API_KEY,
    base_url=OPENAI_BASE_URL
)

# --- åœºæ™¯é…ç½®åŠ è½½ ---
def load_scenarios(file_path: str = None) -> Dict:
    """åŠ è½½åœºæ™¯é…ç½®"""
    if file_path is None:
        file_path = SCENARIOS_FILE
    
    # å¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•å¤šä¸ªå¯èƒ½çš„ä½ç½®
    if not Path(file_path).is_absolute():
        possible_paths = [
            file_path,  # å½“å‰ç›®å½•
            Path(__file__).parent / file_path,  # è„šæœ¬æ‰€åœ¨ç›®å½•
            Path(__file__).parent.parent.parent / file_path,  # é¡¹ç›®æ ¹ç›®å½•
        ]
        for path in possible_paths:
            if Path(path).exists():
                file_path = str(path)
                break
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"åœºæ™¯é…ç½®æ–‡ä»¶ {file_path} æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ parse_document.py")
        print(f"å°è¯•çš„è·¯å¾„: {file_path}")
        return {}

# åœºæ™¯é…ç½®ï¼ˆä»æ–‡æ¡£è§£ææˆ–æ‰‹åŠ¨å®šä¹‰ï¼‰
SCENARIOS = load_scenarios()

# --- SOP æ˜ å°„ä¸åŠ è½½ ---
# å°†ä¸­æ–‡å¤§ç±»æ˜ å°„åˆ°è‹±æ–‡ç›®å½•ï¼ˆæ™ºèƒ½å®¢æœç³»ç»Ÿ/sop_data_global_en ä¸‹ï¼‰
CATEGORY_MAP = {
    "å–æ¬¾/æç°": "withdrawals",
    "å­˜æ¬¾/å……å€¼": "deposits_top_ups",
    "è´¦å·ä¸å®‰å…¨": "account_security",
    "çº¢åˆ© / è¿”æ°´ / VIP æƒç›Š": "bonuses_cashback_vip_benefits",
    "èº«ä»½éªŒè¯ä¸åˆè§„": "identity_verification_compliance",
    "æ¸¸æˆä¸æŠ•æ³¨è§„åˆ™": "game_betting_rules",
    "æ”¯ä»˜æ–¹å¼ä¸èµ„é‡‘æ¸ é“": "payment_methods_funding_channels",
    "æ²Ÿé€šæ¸ é“ä¸è´¦æˆ·æœåŠ¡": "communication_channels_account_services",
    "æ´»åŠ¨ä¸ä¿ƒé”€": "promotions_campaigns",
    "æŠ•è¯‰ä¸äº‰è®®è§£å†³": "platform_rules_general_information",
    "å¹³å°è§„åˆ™ä¸é€šç”¨ä¿¡æ¯": "platform_rules_general_information",
    "ç³»ç»Ÿ / æŠ€æœ¯é—®é¢˜": "system_technical_issues",
    "è´Ÿè´£ä»»åšå½©ä¸è‡ªæˆ‘é™åˆ¶": "platform_rules_general_information",  # è‹¥æ— ä¸“é—¨ç›®å½•ï¼Œæ˜ å°„åˆ°é€šç”¨
}


def load_sops(base_dir: str = None) -> Dict[str, List[Dict[str, str]]]:
    """é¢„åŠ è½½ SOP æ–‡ä»¶å†…å®¹"""
    if base_dir is None:
        base_dir = SOP_BASE_DIR
    
    # å¦‚æœè·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•å¤šä¸ªå¯èƒ½çš„ä½ç½®
    base = Path(base_dir)
    if not base.is_absolute():
        possible_paths = [
            base,  # å½“å‰ç›®å½•
            Path(__file__).parent / base,  # è„šæœ¬æ‰€åœ¨ç›®å½•
            Path(__file__).parent.parent / base,  # learn ç›®å½•
        ]
        for path in possible_paths:
            if path.exists():
                base = path
                break
    data: Dict[str, List[Dict[str, str]]] = {}
    for cat_cn, folder in CATEGORY_MAP.items():
        cat_dir = base / folder
        if not cat_dir.exists():
            continue
        files = []
        for p in cat_dir.rglob("*.md"):
            try:
                text = p.read_text(encoding="utf-8")
            except Exception:
                continue
            files.append({"path": str(p), "name": p.stem, "text": text})
        data[cat_cn] = files
    return data


SOP_DATA = load_sops()


def retrieve_sop(category: str, subcategory: str, k: int = 2) -> List[Dict[str, str]]:
    """åŸºäºç±»åˆ«ä¸å­ç±»åçš„ç®€å•æ£€ç´¢"""
    files = SOP_DATA.get(category, [])
    if not files:
        return []
    key = (subcategory or "").replace(" ", "").lower()
    scored = []
    for f in files:
        name = f["name"].lower()
        score = 0
        if key and key in name:
            score += 2
        scored.append((score, f))
    scored.sort(key=lambda x: x[0], reverse=True)
    hits = [f for s, f in scored[:k] if s > 0]
    if not hits:
        hits = [f for s, f in scored[:k]]
    return hits


def classify_scenario(state: CustomerSupportState) -> Dict:
    """
    èŠ‚ç‚¹1: åˆ†ç±»åœºæ™¯
    è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢å±äºå“ªä¸ªå¤§ç±»åœºæ™¯å’Œå­ç±»åœºæ™¯
    """
    from langchain_core.prompts import ChatPromptTemplate
    # å°è¯•å¤šç§å¯¼å…¥æ–¹å¼ä»¥å…¼å®¹ä¸åŒç‰ˆæœ¬
    try:
        from pydantic import BaseModel, Field
    except ImportError:
        try:
            from langchain_core.pydantic_v1 import BaseModel, Field
        except ImportError:
            from pydantic.v1 import BaseModel, Field
    
    class ScenarioClassification(BaseModel):
        category: str = Field(description="å¤§ç±»åœºæ™¯åç§°")
        subcategory: str = Field(description="å­ç±»åœºæ™¯åç§°")
        confidence: float = Field(description="åˆ†ç±»ç½®ä¿¡åº¦")
    
    # æ„å»ºå¯ç”¨åœºæ™¯åˆ—è¡¨
    available_scenarios = []
    for cat, info in SCENARIOS.items():
        for subcat in info.get('subcategories', []):
            available_scenarios.append(f"{cat} - {subcat['name']}")
    
    # æ·»åŠ ç‰¹æ®Šåœºæ™¯å¤„ç†è¯´æ˜
    special_cases_note = """
ç‰¹æ®Šè¯´æ˜ï¼š
- å¦‚æœæ˜¯é—®å€™ã€é—²èŠï¼ˆå¦‚"ä½ å¥½"ã€"è°¢è°¢"ã€"å†è§"ç­‰ï¼‰ï¼Œåˆ†ç±»ä¸ºï¼šå…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰
- å¦‚æœæ˜¯éä¸šåŠ¡ç›¸å…³çš„é—²èŠï¼Œåˆ†ç±»ä¸ºï¼šå…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰
- å¦‚æœæ— æ³•ç¡®å®šå…·ä½“åœºæ™¯ï¼Œåˆ†ç±»ä¸ºï¼šå…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰
"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ä½ æ˜¯ä¸€ä¸ªå®¢æœåœºæ™¯åˆ†ç±»ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­å±äºä»¥ä¸‹å“ªä¸ªåœºæ™¯ï¼š

å¯ç”¨åœºæ™¯ï¼š
{chr(10).join(f"- {s}" for s in available_scenarios)}
{special_cases_note}

è¯·å‡†ç¡®åˆ†ç±»ç”¨æˆ·æŸ¥è¯¢ã€‚å¦‚æœæ˜¯é—®å€™ã€é—²èŠæˆ–æ— æ³•åˆ†ç±»çš„æŸ¥è¯¢ï¼Œè¯·åˆ†ç±»ä¸º"å…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰"ã€‚"""),
        ("user", "ç”¨æˆ·æŸ¥è¯¢ï¼š{query}")
    ])
    
    classifier = prompt | llm.with_structured_output(ScenarioClassification)
    result = classifier.invoke({"query": state["user_query"]})
    
    return {
        "category": result.category,
        "subcategory": result.subcategory,
        "context": {"confidence": result.confidence}
    }

def get_scenario_flow(state: CustomerSupportState) -> Dict:
    """
    èŠ‚ç‚¹2: è·å–åœºæ™¯æµç¨‹
    æ ¹æ®åˆ†ç±»ç»“æœï¼Œè·å–å¯¹åº”çš„å¤„ç†æµç¨‹
    """
    category = state["category"]
    subcategory = state["subcategory"]
    
    # æŸ¥æ‰¾å¯¹åº”çš„æµç¨‹
    flow_info = None
    if category in SCENARIOS:
        for subcat in SCENARIOS[category].get('subcategories', []):
            if subcat['name'] == subcategory:
                flow_info = subcat
                break
    
    if not flow_info:
        return {
            "context": {
                **state.get("context", {}),
                "error": f"æœªæ‰¾åˆ°åœºæ™¯æµç¨‹: {category} - {subcategory}"
            },
            "next_action": "fallback"
        }
    
    return {
        "context": {
            **state.get("context", {}),
            "flow_info": flow_info,
            "category_description": SCENARIOS[category].get('description', '')
        },
        "next_action": "process"
    }

def process_scenario(state: CustomerSupportState) -> Dict:
    """
    èŠ‚ç‚¹3: å¤„ç†åœºæ™¯
    æ ¹æ®åœºæ™¯æµç¨‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    flow_info = state["context"].get("flow_info", {}) or {}
    category_desc = state["context"].get("category_description", "")
    subcategory_desc = flow_info.get("description", "")
    related = flow_info.get("related_subcategories", []) or []
    
    # åœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›å…³è”å­ç±»ä½œä¸ºä¸Šä¸‹æ–‡å‚è€ƒï¼Œä½†ä¸è®© LLM ç›´æ¥è¾“å‡ºåˆ—è¡¨
    # è¿™æ · LLM å¯ä»¥å‚è€ƒè¿™äº›ä¿¡æ¯ç”Ÿæˆæ›´å‡†ç¡®çš„å›å¤ï¼Œä½†ä¸ä¼šé‡å¤åˆ—å‡º
    related_context = ""
    if related:
        related_context = f"\n\næ³¨æ„ï¼šä»¥ä¸‹æ˜¯ä¸å½“å‰åœºæ™¯ç›¸å…³çš„å…¶ä»–é—®é¢˜ç±»å‹ï¼ˆä¾›å‚è€ƒï¼Œæ— éœ€åœ¨å›å¤ä¸­åˆ—å‡ºï¼‰ï¼š\n" + "\n".join(f"- {item}" for item in related[:3])  # åªæä¾›å‰3ä¸ªä½œä¸ºä¸Šä¸‹æ–‡

    # æ£€ç´¢ SOPï¼ˆæŒ‰å¤§ç±»/å­ç±»ï¼‰ï¼Œæˆªå–ç‰‡æ®µä»¥æ§é•¿åº¦
    sop_hits = retrieve_sop(state["category"], state["subcategory"], k=2)
    sop_text = ""
    if sop_hits:
        snippets = []
        for h in sop_hits:
            snippet = h["text"][:800]
            snippets.append(f"ã€{h['name']}ã€‘\n{snippet}")
        sop_text = "\n\nSOPå‚è€ƒï¼ˆèŠ‚é€‰ï¼‰ï¼š\n" + "\n\n".join(snippets)

    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ï¼Œä¸“é—¨å¤„ç†ä»¥ä¸‹åœºæ™¯ï¼š

å¤§ç±»åœºæ™¯ï¼š{state['category']}
{category_desc}

å­ç±»åœºæ™¯ï¼š{state['subcategory']}
{subcategory_desc}
{related_context}
{sop_text}

è¯·æ ¹æ®åœºæ™¯æµç¨‹å’Œç”¨æˆ·æŸ¥è¯¢ï¼Œæä¾›ä¸“ä¸šã€å‹å¥½çš„å›å¤ã€‚
é‡è¦ï¼šåªéœ€å›ç­”ç”¨æˆ·çš„å…·ä½“é—®é¢˜ï¼Œä¸è¦åˆ—å‡º"å…³è”å­ç±»"æˆ–"ç›¸å…³é—®é¢˜"ç­‰åˆ—è¡¨ã€‚"""),
        ("user", "ç”¨æˆ·æŸ¥è¯¢ï¼š{query}\n\nè¯·æä¾›å›å¤ã€‚")
    ])
    
    chain = prompt | llm
    response = chain.invoke({"query": state["user_query"]})

    # æ¸…ç†å›å¤å†…å®¹ï¼Œç§»é™¤å¯èƒ½å‡ºç°çš„å…³è”å­ç±»åˆ—è¡¨
    response_text = response.content
    
    # ç§»é™¤å›å¤ä¸­å¯èƒ½å‡ºç°çš„å…³è”å­ç±»åˆ—è¡¨ï¼ˆå¦‚æœ LLM è¿˜æ˜¯è¾“å‡ºäº†ï¼‰
    lines = response_text.split('\n')
    cleaned_lines = []
    skip_until_empty = False
    for i, line in enumerate(lines):
        # æ£€æµ‹æ˜¯å¦å¼€å§‹å‡ºç°å…³è”å­ç±»åˆ—è¡¨
        if any(keyword in line for keyword in ["å…³è”å­ç±»", "ç›¸å…³é—®é¢˜", "ç›¸å…³å¸®åŠ©", "ğŸ’¡ ç›¸å…³", "ç›¸å…³ä¸»é¢˜"]):
            skip_until_empty = True
            continue
        # å¦‚æœé‡åˆ°ç©ºè¡Œï¼Œåœæ­¢è·³è¿‡
        if skip_until_empty and line.strip() == "":
            skip_until_empty = False
            continue
        # å¦‚æœæ­£åœ¨è·³è¿‡ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨é¡¹
        if skip_until_empty:
            if line.strip().startswith("-") or line.strip().startswith("â€¢") or line.strip().startswith("*"):
                continue
            else:
                skip_until_empty = False
        
        cleaned_lines.append(line)
    
    response_text = '\n'.join(cleaned_lines).strip()
    
    # ä¸å†åœ¨å›å¤ä¸­æ˜¾ç¤ºå…³è”å­ç±»ï¼ˆç”¨æˆ·è¦æ±‚ï¼‰
    # å…³è”å­ç±»åªåœ¨ç³»ç»Ÿæç¤ºä¸­ä½œä¸ºä¸Šä¸‹æ–‡å‚è€ƒï¼Œå¸®åŠ© LLM ç”Ÿæˆæ›´å‡†ç¡®çš„å›å¤
    
    # SOP å‚è€ƒèµ„æ–™ï¼ˆå¯é€‰ï¼šå¦‚æœä¸éœ€è¦ä¹Ÿå¯ä»¥æ³¨é‡Šæ‰ï¼‰
    # if sop_hits:
    #     refs = "\n".join(f"   â€¢ {h['name']}" for h in sop_hits)
    #     response_text = f"{response_text}\n\nğŸ“š å‚è€ƒèµ„æ–™ï¼š\n{refs}"
    
    return {
        "response": response_text,
        "next_action": "complete"
    }

def fallback_handler(state: CustomerSupportState) -> Dict:
    """
    èŠ‚ç‚¹4: é™çº§å¤„ç†
    å½“æ— æ³•è¯†åˆ«åœºæ™¯æ—¶çš„é»˜è®¤å¤„ç†ï¼ˆåŒ…æ‹¬é—²èŠã€é—®å€™ã€å…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼‰
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    query = state["user_query"].lower()
    
    # æ£€æµ‹æ˜¯å¦æ˜¯é—®å€™æˆ–é—²èŠ
    greetings = ["ä½ å¥½", "hello", "hi", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½", "æ‚¨å¥½"]
    farewells = ["å†è§", "bye", "æ‹œæ‹œ", "è°¢è°¢", "thank", "æ„Ÿè°¢"]
    small_talk = ["åœ¨å—", "æœ‰äººå—", "å®¢æœ", "äººå·¥", "help"]
    
    is_greeting = any(g in query for g in greetings)
    is_farewell = any(f in query for f in farewells)
    is_small_talk = any(s in query for s in small_talk)
    
    # æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
    if is_greeting:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å®¢æœåŠ©æ‰‹ã€‚ç”¨æˆ·å‘é€äº†é—®å€™ï¼Œè¯·ç¤¼è²Œåœ°å›åº”å¹¶è¯¢é—®å¦‚ä½•å¸®åŠ©ã€‚
å›å¤è¦æ±‚ï¼š
1. å‹å¥½åœ°å›åº”é—®å€™
2. ç®€è¦ä»‹ç»ä½ å¯ä»¥æä¾›çš„å¸®åŠ©
3. è¯¢é—®ç”¨æˆ·éœ€è¦ä»€ä¹ˆå¸®åŠ©
4. ä¿æŒç®€æ´ï¼Œä¸è¶…è¿‡3å¥è¯"""
    elif is_farewell:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å®¢æœåŠ©æ‰‹ã€‚ç”¨æˆ·å‘é€äº†å‘Šåˆ«æˆ–æ„Ÿè°¢ï¼Œè¯·ç¤¼è²Œåœ°å›åº”ã€‚
å›å¤è¦æ±‚ï¼š
1. å‹å¥½åœ°å›åº”
2. è¡¨è¾¾æ„¿æ„ç»§ç»­æä¾›å¸®åŠ©
3. ä¿æŒç®€æ´ï¼Œ1-2å¥è¯å³å¯"""
    elif is_small_talk:
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„å®¢æœåŠ©æ‰‹ã€‚ç”¨æˆ·å¯èƒ½åœ¨æµ‹è¯•æˆ–å¯»æ‰¾å¸®åŠ©ï¼Œè¯·å‹å¥½åœ°å›åº”å¹¶å¼•å¯¼ã€‚
å›å¤è¦æ±‚ï¼š
1. å‹å¥½åœ°ç¡®è®¤åœ¨çº¿
2. è¯´æ˜å¯ä»¥æä¾›å¸®åŠ©
3. è¯¢é—®å…·ä½“éœ€æ±‚"""
    else:
        # å…¶ä»–æœªåˆ†ç±»é—®é¢˜
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚è™½ç„¶æ— æ³•å‡†ç¡®è¯†åˆ«ç”¨æˆ·çš„å…·ä½“åœºæ™¯ï¼Œä½†è¯·å°½åŠ›æä¾›å¸®åŠ©ã€‚
å›å¤è¦æ±‚ï¼š
1. å‹å¥½åœ°å›åº”
2. å°è¯•ç†è§£ç”¨æˆ·éœ€æ±‚
3. å¦‚æœæ— æ³•ç¡®å®šï¼Œå¯ä»¥è¯¢é—®æ›´å¤šä¿¡æ¯æˆ–å¼•å¯¼ç”¨æˆ·æè¿°å…·ä½“é—®é¢˜
4. å¯ä»¥æåŠå¸¸è§é—®é¢˜ç±»å‹ï¼ˆå¦‚è´¦æˆ·ã€å……å€¼ã€æç°ç­‰ï¼‰å¸®åŠ©ç”¨æˆ·æ˜ç¡®éœ€æ±‚"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("user", "ç”¨æˆ·æŸ¥è¯¢ï¼š{query}\n\nè¯·æä¾›å›å¤ã€‚")
    ])
    
    chain = prompt | llm
    response = chain.invoke({"query": state["user_query"]})
    
    return {
        "response": response.content,
        "next_action": "complete",
        "category": state.get("category", "å…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰"),
        "subcategory": "é€šç”¨å›å¤"
    }

def route_after_classification(state: CustomerSupportState) -> Literal["get_flow", "fallback"]:
    """è·¯ç”±ï¼šåˆ†ç±»åå†³å®šä¸‹ä¸€æ­¥"""
    # å¦‚æœæ˜ç¡®æ ‡è®°ä½¿ç”¨ fallbackï¼Œæˆ–è€…åˆ†ç±»ä¸º"å…¶ä»–æœªåˆ†ç±»é—®é¢˜ï¼ˆå…œåº•ï¼‰"ï¼Œç›´æ¥èµ° fallback
    context = state.get("context", {})
    if context.get("use_fallback") or "å…¶ä»–æœªåˆ†ç±»é—®é¢˜" in state.get("category", "") or "å…œåº•" in state.get("category", ""):
        return "fallback"
    
    # å¦‚æœæœ‰æœ‰æ•ˆçš„åˆ†ç±»ç»“æœï¼Œèµ°æ­£å¸¸æµç¨‹
    if state.get("category") and state.get("subcategory"):
        return "get_flow"
    
    # é»˜è®¤èµ° fallback
    return "fallback"

def route_after_flow(state: CustomerSupportState) -> Literal["process", "fallback"]:
    """è·¯ç”±ï¼šè·å–æµç¨‹åå†³å®šä¸‹ä¸€æ­¥"""
    if state.get("context", {}).get("flow_info"):
        return "process"
    return "fallback"

def route_after_process(state: CustomerSupportState) -> Literal["complete", "fallback"]:
    """è·¯ç”±ï¼šå¤„ç†åå†³å®šä¸‹ä¸€æ­¥"""
    if state.get("response"):
        return "complete"
    return "fallback"

# æ„å»ºå›¾
def build_customer_support_graph():
    """æ„å»ºå®¢æœåœºæ™¯å¤„ç†å›¾"""
    builder = StateGraph(CustomerSupportState)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("classify", classify_scenario)
    builder.add_node("get_flow", get_scenario_flow)
    builder.add_node("process", process_scenario)
    builder.add_node("fallback", fallback_handler)
    
    # æ·»åŠ è¾¹
    builder.add_edge(START, "classify")
    
    # æ¡ä»¶è·¯ç”±
    builder.add_conditional_edges(
        "classify",
        route_after_classification,
        {
            "get_flow": "get_flow",
            "fallback": "fallback"
        }
    )
    
    builder.add_conditional_edges(
        "get_flow",
        route_after_flow,
        {
            "process": "process",
            "fallback": "fallback"
        }
    )
    
    builder.add_conditional_edges(
        "process",
        route_after_process,
        {
            "complete": END,
            "fallback": "fallback"
        }
    )
    
    builder.add_edge("fallback", END)
    
    # ç¼–è¯‘å›¾ï¼ˆæ·»åŠ æŒä¹…åŒ–ï¼‰
    memory = MemorySaver()
    graph = builder.compile(checkpointer=memory)
    
    return graph

# åˆ›å»ºå›¾å®ä¾‹
customer_support_graph = build_customer_support_graph()

if __name__ == "__main__":
    import uuid
    
    # æµ‹è¯•è¿è¡Œ
    graph = build_customer_support_graph()
    
    test_query = "æˆ‘æƒ³æŸ¥è¯¢è®¢å•çŠ¶æ€"
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    result = graph.invoke(
        {
            "messages": [],
            "user_query": test_query,
            "category": "",
            "subcategory": "",
            "context": {},
            "response": "",
            "next_action": "",
            "history": []
        },
        config
    )
    
    print(f"ç”¨æˆ·æŸ¥è¯¢: {test_query}")
    print(f"åˆ†ç±»ç»“æœ: {result['category']} - {result['subcategory']}")
    print(f"å›å¤: {result['response']}")

